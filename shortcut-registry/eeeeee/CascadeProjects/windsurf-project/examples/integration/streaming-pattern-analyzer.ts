#!/usr/bin/env bun

// Production-Hardened Streaming Pattern Analyzer with Zero Memory Bloat
export {}; // Make this a module

// Import Database type from bun:sqlite
interface Database {
  run(sql: string, ...args: any[]): void;
  prepare(sql: string): {
    run(...args: any[]): void;
    all(): any[];
    get(): any;
  };
  query(sql: string): {
    all(): any[];
    get(): any;
  };
  close(): void;
}

interface PatternData {
  pattern: string;
  security_score: number;
  redos_score: number;
  last_analyzed: number;
}

interface StreamAnalysisOptions {
  inputFile: string;
  cacheDb: string;
  workerThreads: number;
  chunkSize: number;
}

class StreamingURLPatternAnalyzer {
  private db: Database;
  private cache: Map<string, PatternData> = new Map();
  private workerPool: Worker[] = [];
  
  constructor(options: StreamAnalysisOptions) {
    const { Database } = require('bun:sqlite');
    this.db = new Database(options.cacheDb);
    this.initializeCache();
    this.initializeWorkers(options.workerThreads);
  }

  private initializeCache() {
    this.db.run(`CREATE TABLE IF NOT EXISTS cached_results (
      pattern_hash TEXT PRIMARY KEY,
      security_score INTEGER,
      redos_score REAL,
      last_analyzed INTEGER,
      pattern_text TEXT
    )`);

    // Load existing cache into memory
    const cached = this.db.query('SELECT * FROM cached_results').all() as PatternData[];
    cached.forEach(item => {
      this.cache.set(item.pattern, item);
    });
  }

  private initializeWorkers(threadCount: number) {
    for (let i = 0; i < threadCount; i++) {
      const worker = new Worker('./pattern-analysis-worker.ts');
      this.workerPool.push(worker);
    }
  }

  async analyzePatterns(options: StreamAnalysisOptions): Promise<{
    totalProcessed: number;
    cacheHits: number;
    securityIssues: number;
    redosRisks: number;
  }> {
    const { Database } = require('bun:sqlite');
    const file = Bun.file(options.inputFile);
    
    if (!file.exists()) {
      throw new Error(`Input file ${options.inputFile} does not exist`);
    }

    const stream = file.stream();
    const reader = stream.getReader();
    const decoder = new TextDecoder();
    
    let buffer = '';
    let totalProcessed = 0;
    let cacheHits = 0;
    let securityIssues = 0;
    let redosRisks = 0;

    const stmt = this.db.prepare(`
      INSERT OR REPLACE INTO cached_results 
      (pattern_hash, security_score, redos_score, last_analyzed, pattern_text)
      VALUES (?, ?, ?, ?, ?)
    `);

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (!line.trim()) continue;
          
          try {
            const patternData = JSON.parse(line) as { pattern: string };
            const patternHash = Bun.hash(patternData.pattern).toString();
            
            // Check cache first
            if (this.cache.has(patternData.pattern)) {
              cacheHits++;
              continue;
            }

            // Analyze pattern (simplified for demo)
            const analysis = await this.analyzePattern(patternData.pattern);
            
            // Cache results
            const cachedData: PatternData = {
              pattern: patternData.pattern,
              ...analysis,
              last_analyzed: Date.now()
            };
            
            this.cache.set(patternData.pattern, cachedData);
            stmt.run(
              patternHash,
              analysis.security_score,
              analysis.redos_score,
              Date.now(),
              patternData.pattern
            );

            if (analysis.security_score < 5) securityIssues++;
            if (analysis.redos_score > 0.7) redosRisks++;
            
            totalProcessed++;
          } catch (error) {
            console.error('Error processing line:', line, error);
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    return {
      totalProcessed,
      cacheHits,
      securityIssues,
      redosRisks
    };
  }

  private async analyzePattern(pattern: string): Promise<{
    security_score: number;
    redos_score: number;
  }> {
    // Simplified security analysis
    let securityScore = 10;
    let redosScore = 0;

    // Check for dangerous patterns
    if (pattern.includes('..')) securityScore -= 3;
    if (pattern.includes('169.254.169.254')) securityScore -= 5;
    if (pattern.includes('${USER}')) securityScore -= 2;
    
    // Check for potential ReDoS
    const nestedGroups = (pattern.match(/\(/g) || []).length;
    const quantifiers = (pattern.match(/\*|\+|\?/g) || []).length;
    redosScore = Math.min((nestedGroups * quantifiers) / 20, 1);

    return {
      security_score: Math.max(0, securityScore),
      redos_score: redosScore
    };
  }

  generateReport(): void {
    const stats = this.db.query(`
      SELECT 
        COUNT(*) as total_patterns,
        AVG(security_score) as avg_security,
        AVG(redos_score) as avg_redos,
        COUNT(CASE WHEN security_score < 5 THEN 1 END) as high_risk,
        COUNT(CASE WHEN redos_score > 0.7 THEN 1 END) as redos_risk
      FROM cached_results
    `).get() as any;

    console.log('ðŸ“Š Pattern Analysis Report');
    console.log('==========================');
    console.log(`Total Patterns: ${stats.total_patterns}`);
    console.log(`Avg Security Score: ${stats.avg_security?.toFixed(2)}`);
    console.log(`Avg ReDoS Score: ${stats.avg_redos?.toFixed(3)}`);
    console.log(`High Risk Patterns: ${stats.high_risk}`);
    console.log(`ReDoS Risk Patterns: ${stats.redos_risk}`);
  }

  cleanup(): void {
    this.workerPool.forEach(worker => worker.terminate());
    this.db.close();
  }
}

// CLI interface
async function main() {
  const args = process.argv.slice(2);
  const options: StreamAnalysisOptions = {
    inputFile: args.find(arg => arg.startsWith('--input='))?.split('=')[1] || 'patterns.ndjson',
    cacheDb: args.find(arg => arg.startsWith('--cache-db='))?.split('=')[1] || 'results.sqlite',
    workerThreads: parseInt(args.find(arg => arg.startsWith('--worker-threads='))?.split('=')[1] || '4'),
    chunkSize: parseInt(args.find(arg => arg.startsWith('--chunk-size='))?.split('=')[1] || '65536')
  };

  console.log('ðŸš€ Production Pattern Analyzer');
  console.log('==============================');
  console.log(`Input: ${options.inputFile}`);
  console.log(`Cache: ${options.cacheDb}`);
  console.log(`Workers: ${options.workerThreads}`);
  console.log(`Chunk Size: ${options.chunkSize} bytes`);

  const analyzer = new StreamingURLPatternAnalyzer(options);
  
  try {
    const startTime = Bun.nanoseconds();
    const results = await analyzer.analyzePatterns(options);
    const duration = (Bun.nanoseconds() - startTime) / 1e9;

    console.log('\nâœ… Analysis Complete');
    console.log(`Duration: ${duration.toFixed(2)}s`);
    console.log(`Processed: ${results.totalProcessed}`);
    console.log(`Cache Hits: ${results.cacheHits}`);
    console.log(`Security Issues: ${results.securityIssues}`);
    console.log(`ReDoS Risks: ${results.redosRisks}`);

    analyzer.generateReport();
  } finally {
    analyzer.cleanup();
  }
}

export { StreamingURLPatternAnalyzer };

// Check if this file is being run directly
if (require.main === module) {
  main().catch(console.error);
}
