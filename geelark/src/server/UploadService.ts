/**
 * Upload Service - S3/R2 Cloud Storage Integration
 *
 * Feature-flagged implementation using Bun's File API patterns:
 * - Bun.file() for lazy file handles
 * - .json() for config parsing
 * - .arrayBuffer() + Buffer.from() for binary uploads
 *
 * Compile-time elimination ensures 0% overhead when features are disabled.
 */

import { feature } from "bun:bundle";
import { UPLOAD } from "./ServerConstants.js";

/**
 * Upload configuration
 */
export interface UploadConfig {
  /** S3/R2 provider */
  provider: "s3" | "r2" | "local";
  /** AWS access key ID */
  accessKeyId: string;
  /** AWS secret access key */
  secretAccessKey: string;
  /** S3 bucket name */
  bucket: string;
  /** AWS region (for S3) */
  region?: string;
  /** Custom endpoint (for R2 or S3-compatible) */
  endpoint?: string;
}

/**
 * Upload options
 */
export interface UploadOptions {
  /** Original filename */
  filename: string;
  /** Content type */
  contentType: string;
  /** Content disposition */
  contentDisposition?: "inline" | "attachment";
  /** Custom metadata (requires FEAT_CUSTOM_METADATA) */
  metadata?: Record<string, string>;
}

/**
 * Upload progress tracking
 */
export interface UploadProgress {
  uploadId: string;
  filename: string;
  totalBytes: number;
  uploadedBytes: number;
  progress: number;
  status: "initiated" | "uploading" | "completed" | "failed" | "cancelled";
  startedAt: number;
  completedAt?: number;
  error?: string;
  url?: string;
}

/**
 * Upload result
 */
export interface UploadResult {
  uploadId: string;
  filename: string;
  url: string;
  size: number;
  duration: number;
  provider: "s3" | "r2" | "local";
}

/**
 * Upload Service class
 *
 * Manages file uploads to S3/R2 with feature-flagged capabilities.
 * Disabled features are eliminated at compile time for zero runtime overhead.
 */
export class UploadService {
  private config: UploadConfig;
  private activeUploads: Map<string, UploadProgress> = new Map();

  /**
   * S3 client (only compiled if FEAT_CLOUD_UPLOAD enabled)
   * This is compile-time eliminated if the feature is disabled
   */
  private s3Client: any;

  constructor(config: UploadConfig) {
    this.config = config;

    // Initialize S3 client only if FEAT_CLOUD_UPLOAD enabled
    // This code is completely eliminated if feature is disabled
    if (feature("FEAT_CLOUD_UPLOAD", false) && this.config.provider !== "local") {
      // Dynamic import of S3 client (only when feature enabled)
      const { s3 } = require("bun");

      this.s3Client = s3({
        accessKeyId: this.config.accessKeyId,
        secretAccessKey: this.config.secretAccessKey,
        bucket: this.config.bucket,
        region: this.config.region,
        endpoint: this.config.endpoint,
      });

      console.log(`‚úÖ S3/R2 client initialized: ${this.config.provider}/${this.config.bucket}`);
    } else {
      console.log(`üìÅ Local upload mode: ${this.config.localDir || "./uploads"}`);
    }
  }

  /**
   * Initiate upload - chooses strategy based on file size and features
   */
  async initiateUpload(
    file: File | Blob,
    options: UploadOptions
  ): Promise<UploadResult> {
    const uploadId = crypto.randomUUID();
    const totalBytes = file.size;
    const startedAt = Date.now();

    const progress: UploadProgress = {
      uploadId,
      filename: options.filename,
      totalBytes,
      uploadedBytes: 0,
      progress: 0,
      status: "initiated",
      startedAt,
    };

    this.activeUploads.set(uploadId, progress);

    // Validate file size
    if (totalBytes > UPLOAD.MAX_MULTIPART_SIZE) {
      throw new Error(`File size exceeds maximum of ${UPLOAD.MAX_MULTIPART_SIZE} bytes`);
    }

    // Choose upload strategy based on file size and features
    const HAS_MULTIPART = feature("FEAT_MULTIPART_UPLOAD", false);

    if (HAS_MULTIPART && totalBytes > UPLOAD.MAX_SIMPLE_SIZE) {
      console.log(`üß© Using multipart upload for ${options.filename}`);
      return this.multipartUpload(uploadId, file, options, startedAt);
    } else {
      console.log(`üì¶ Using simple upload for ${options.filename}`);
      return this.simpleUpload(uploadId, file, options, startedAt);
    }
  }

  /**
   * Simple upload for small files (<= 5MB)
   * Uses Bun File API: .arrayBuffer() + Buffer.from()
   */
  private async simpleUpload(
    uploadId: string,
    file: File | Blob,
    options: UploadOptions,
    startedAt: number
  ): Promise<UploadResult> {
    const progress = this.activeUploads.get(uploadId);
    if (!progress) throw new Error("Upload not found");

    progress.status = "uploading";

    try {
      let url: string;
      const key = this.generateKey(options.filename);

      // Convert file to Buffer using Bun File API pattern
      const arrayBuffer = await file.arrayBuffer();
      const buffer = Buffer.from(arrayBuffer);

      console.log(`üìÑ File converted to Buffer: ${buffer.length} bytes`);

      if (this.config.provider === "local") {
        // Local development: store to disk
        url = await this.uploadToLocal(key, buffer, options);
      } else {
        // Cloud upload to S3/R2
        if (!this.s3Client) {
          throw new Error("S3 client not initialized");
        }

        // Prepare metadata
        const metadata: Record<string, string> = {};

        // Add custom metadata if feature enabled
        const HAS_CUSTOM_METADATA = feature("FEAT_CUSTOM_METADATA", false);
        if (HAS_CUSTOM_METADATA && options.metadata) {
          Object.assign(metadata, options.metadata);
          console.log(`üè∑Ô∏è  Custom metadata: ${JSON.stringify(options.metadata)}`);
        }

        // Add system metadata
        metadata[UPLOAD.METADATA_KEYS.ORIGINAL_FILENAME] = options.filename;
        metadata[UPLOAD.METADATA_KEYS.CONTENT_TYPE] = options.contentType;
        metadata[UPLOAD.METADATA_KEYS.FILE_SIZE] = file.size.toString();
        metadata[UPLOAD.METADATA_KEYS.UPLOADED_AT] = new Date().toISOString();

        // Upload to S3/R2
        await this.s3Client.write(key, arrayBuffer, {
          type: options.contentType,
          disposition: options.contentDisposition || UPLOAD.DISPOSITION.INLINE,
          metadata,
        });

        console.log(`‚òÅÔ∏è  Uploaded to ${this.config.provider}/${this.config.bucket}/${key}`);

        // Generate public URL
        url = this.generateUrl(key);
      }

      const completedAt = Date.now();
      const duration = completedAt - startedAt;

      // Update progress
      progress.uploadedBytes = file.size;
      progress.progress = 100;
      progress.status = "completed";
      progress.completedAt = completedAt;
      progress.url = url;

      console.log(`‚úÖ Upload complete: ${options.filename} (${duration}ms)`);

      // Record telemetry if feature enabled
      this.recordTelemetry({
        uploadId,
        filename: options.filename,
        fileSize: file.size,
        duration,
        status: "success",
        provider: this.config.provider,
        timestamp: startedAt,
      });

      return {
        uploadId,
        filename: options.filename,
        url,
        size: file.size,
        duration,
        provider: this.config.provider,
      };
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      progress.status = "failed";
      progress.error = errorMessage;

      this.recordTelemetry({
        uploadId,
        filename: options.filename,
        fileSize: file.size,
        duration: Date.now() - startedAt,
        status: "failure",
        provider: this.config.provider,
        timestamp: startedAt,
      });

      throw error;
    }
  }

  /**
   * Multipart upload for large files (> 5MB) - Premium feature
   * Requires FEAT_MULTIPART_UPLOAD feature flag
   * This entire method is compile-time eliminated if feature is disabled
   */
  private async multipartUpload(
    uploadId: string,
    file: File | Blob,
    options: UploadOptions,
    startedAt: number
  ): Promise<UploadResult> {
    // Compile-time check: multipart upload only if feature enabled
    if (!feature("FEAT_MULTIPART_UPLOAD", false)) {
      throw new Error("Multipart upload not enabled (FEAT_MULTIPART_UPLOAD)");
    }

    const progress = this.activeUploads.get(uploadId);
    if (!progress) throw new Error("Upload not found");

    progress.status = "uploading";

    try {
      const key = this.generateKey(options.filename);

      // Convert file to Buffer
      const arrayBuffer = await file.arrayBuffer();
      const buffer = Buffer.from(arrayBuffer);

      console.log(`üß© Starting multipart upload: ${buffer.length} bytes`);

      // Calculate chunks
      const totalChunks = Math.ceil(buffer.length / UPLOAD.CHUNK_SIZE);
      let uploadedBytes = 0;

      console.log(`üì¶ Total chunks: ${totalChunks}`);

      // Upload chunks in parallel
      const chunkPromises: Promise<{ partNumber: number; etag: string }>[] = [];

      for (let i = 0; i < totalChunks; i++) {
        const start = i * UPLOAD.CHUNK_SIZE;
        const end = Math.min(start + UPLOAD.CHUNK_SIZE, buffer.length);
        const chunk = buffer.subarray(start, end);
        const partNumber = i + 1;

        console.log(`üì§ Uploading part ${partNumber}/${totalChunks} (${chunk.length} bytes)`);

        chunkPromises.push(
          (async () => {
            // Upload chunk
            const etag = await this.uploadPart(key, partNumber, chunk, options);

            // Update progress
            uploadedBytes += chunk.length;
            if (progress) {
              progress.uploadedBytes = uploadedBytes;
              progress.progress = Math.floor((uploadedBytes / buffer.length) * 100);

              // Emit progress update if feature enabled
              if (feature("FEAT_UPLOAD_PROGRESS", false)) {
                this.emitProgress(uploadId, progress);
              }
            }

            console.log(`‚úÖ Part ${partNumber} complete: ${progress?.progress.toFixed(1)}%`);

            return { partNumber, etag };
          })()
        );
      }

      // Wait for all chunks to complete
      const parts = await Promise.all(chunkPromises);

      // Complete multipart upload
      await this.completeMultipartUpload(key, parts, options);

      const completedAt = Date.now();
      const duration = completedAt - startedAt;

      progress.uploadedBytes = buffer.length;
      progress.progress = 100;
      progress.status = "completed";
      progress.completedAt = completedAt;
      progress.url = this.generateUrl(key);

      console.log(`‚úÖ Multipart upload complete: ${options.filename} (${duration}ms)`);

      this.recordTelemetry({
        uploadId,
        filename: options.filename,
        fileSize: file.size,
        duration,
        status: "success",
        provider: this.config.provider,
        timestamp: startedAt,
      });

      return {
        uploadId,
        filename: options.filename,
        url: progress.url,
        size: file.size,
        duration,
        provider: this.config.provider,
      };
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      progress.status = "failed";
      progress.error = errorMessage;

      this.recordTelemetry({
        uploadId,
        filename: options.filename,
        fileSize: file.size,
        duration: Date.now() - startedAt,
        status: "failure",
        provider: this.config.provider,
        timestamp: startedAt,
      });

      throw error;
    }
  }

  /**
   * Upload a single part in multipart upload
   */
  private async uploadPart(
    key: string,
    partNumber: number,
    data: ArrayBuffer,
    options: UploadOptions
  ): Promise<string> {
    // Simulated part upload (Bun's S3 client doesn't expose multipart API directly yet)
    // In production, this would use S3's UploadPart API
    const etag = crypto.randomUUID();
    return etag;
  }

  /**
   * Complete multipart upload
   */
  private async completeMultipartUpload(
    key: string,
    parts: Array<{ partNumber: number; etag: string }>,
    options: UploadOptions
  ): Promise<void> {
    // Simulated completion (Bun's S3 client doesn't expose multipart API directly yet)
    // In production, this would use S3's CompleteMultipartUpload API
  }

  /**
   * Upload to local filesystem
   */
  private async uploadToLocal(
    key: string,
    buffer: Buffer,
    options: UploadOptions
  ): Promise<string> {
    const localDir = (this.config as any).localDir || "./uploads";
    const filePath = `${localDir}/${key}`;

    // Ensure directory exists
    const dir = filePath.substring(0, filePath.lastIndexOf("/"));
    await Bun.write(`${dir}/.gitkeep`, "");

    // Write file using Bun File API
    await Bun.write(filePath, buffer);

    console.log(`üíæ Uploaded to local: ${filePath}`);

    return filePath;
  }

  /**
   * Generate S3 key from filename
   */
  private generateKey(filename: string): string {
    const timestamp = Date.now();
    const random = crypto.randomUUID().split("-")[0];
    return `uploads/${timestamp}-${random}/${filename}`;
  }

  /**
   * Generate public URL for uploaded file
   */
  private generateUrl(key: string): string {
    if (this.config.endpoint) {
      return `${this.config.endpoint}/${this.config.bucket}/${key}`;
    }

    if (this.config.provider === "s3") {
      const region = this.config.region || "us-east-1";
      return `https://${this.config.bucket}.s3.${region}.amazonaws.com/${key}`;
    }

    // R2 or custom endpoint
    return `https://${key}`;
  }

  /**
   * Get upload progress
   */
  getProgress(uploadId: string): UploadProgress | null {
    return this.activeUploads.get(uploadId) || null;
  }

  /**
   * Get all active uploads
   */
  getActiveUploads(): UploadProgress[] {
    return Array.from(this.activeUploads.values()).filter(
      (u) => u.status === "initiated" || u.status === "uploading"
    );
  }

  /**
   * Cancel upload
   */
  cancelUpload(uploadId: string): boolean {
    const progress = this.activeUploads.get(uploadId);
    if (!progress) return false;

    progress.status = "cancelled";
    return true;
  }

  /**
   * Emit progress update (if FEAT_UPLOAD_PROGRESS enabled)
   * This method is compile-time eliminated if feature is disabled
   */
  private emitProgress(uploadId: string, progress: UploadProgress): void {
    // Compile-time elimination: progress only if FEAT_UPLOAD_PROGRESS enabled
    if (!feature("FEAT_UPLOAD_PROGRESS", false)) {
      return; // Entire method body eliminated if feature disabled
    }

    // Progress emission will be handled by dashboard-server.ts via WebSocket
    console.log(`üìä Upload progress: ${progress.filename} - ${progress.progress.toFixed(1)}%`);
  }

  /**
   * Record upload telemetry (if FEAT_UPLOAD_ANALYTICS enabled)
   * This method is compile-time eliminated if feature is disabled
   */
  private recordTelemetry(data: {
    uploadId: string;
    filename: string;
    fileSize: number;
    duration: number;
    status: "success" | "failure";
    provider: "s3" | "r2" | "local";
    timestamp: number;
  }): void {
    // Compile-time elimination: analytics only if FEAT_UPLOAD_ANALYTICS enabled
    if (!feature("FEAT_UPLOAD_ANALYTICS", false)) {
      return; // Entire method body eliminated if feature disabled
    }

    // Record telemetry
    console.log(`üìä Upload telemetry: ${data.filename} - ${data.status} (${data.duration}ms)`);

    // TelemetrySystem integration will be added separately
    // This is the integration point
  }

  /**
   * Clean up completed uploads
   */
  cleanup(maxAge: number = 3600000): void {
    const now = Date.now();

    for (const [uploadId, progress] of this.activeUploads.entries()) {
      const age = now - (progress.completedAt || progress.startedAt);

      if (
        age > maxAge &&
        (progress.status === "completed" ||
          progress.status === "failed" ||
          progress.status === "cancelled")
      ) {
        this.activeUploads.delete(uploadId);
      }
    }
  }

  /**
   * Format bytes to human readable
   */
  private formatBytes(bytes: number): string {
    if (bytes === 0) return "0 Bytes";

    const k = 1024;
    const sizes = ["Bytes", "KB", "MB", "GB", "TB"];
    const i = Math.floor(Math.log(bytes) / Math.log(k));

    return Math.round(bytes / Math.pow(k, i) * 100) / 100 + " " + sizes[i];
  }
}

// ============================================================================
// Config Loader (uses Bun File API)
// ============================================================================

/**
 * Load upload configuration from JSON file using Bun File API
 * Uses .json() method for async parsing
 */
export async function loadUploadConfig(configPath: string): Promise<UploadConfig> {
  try {
    // Get file handle (lazy - doesn't read yet)
    const configFile = Bun.file(configPath);

    // Check if file exists
    if (!configFile.size) {
      throw new Error(`Config file not found: ${configPath}`);
    }

    // Parse JSON using Bun File API
    const config = await configFile.json() as UploadConfig;

    console.log(`‚úÖ Upload config loaded: ${config.provider}/${config.bucket}`);

    return config;
  } catch (error) {
    console.error(`‚ùå Failed to load upload config: ${error}`);
    throw error;
  }
}

/**
 * Create upload service from environment variables
 */
export function createUploadServiceFromEnv(): UploadService {
  const config: UploadConfig = {
    provider: (process.env.UPLOAD_PROVIDER as "s3" | "r2" | "local") || "local",
    accessKeyId: process.env.S3_ACCESS_KEY_ID || "",
    secretAccessKey: process.env.S3_SECRET_ACCESS_KEY || "",
    bucket: process.env.S3_BUCKET || "uploads",
    region: process.env.S3_REGION,
    endpoint: process.env.S3_ENDPOINT,
    localDir: process.env.UPLOAD_LOCAL_DIR || "./uploads",
  };

  return new UploadService(config);
}
