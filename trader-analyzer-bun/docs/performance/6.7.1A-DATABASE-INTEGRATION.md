# 6.7.1A.0.0.0.0 Database Integration Guide

**Version**: 6.7.1A.0.0.0.0  
**Status**: âœ… **OPERATIONAL**

## Overview

With the infrastructure fix complete, **6.7.1A.0.0.0.0 Statistical Significance Testing** can now access raw performance data from database audit tables and perform statistical comparisons on actual historical data.

### Data Sources

1. **`line_movement_audit_v2`** - Performance metrics from forensic logging
   - `response_size` - Response size in bytes (proxy for execution time)
   - `response_status` - HTTP status codes
   - `timestamp` - Request timestamps
   - `bookmaker` - Bookmaker identifier
   - `eventId` - Event identifier

2. **`url_anomaly_audit`** - Anomaly-related performance data
   - `parsed_param_count` - Parameter count before correction
   - `corrected_param_count` - Parameter count after correction
   - `detected_at` - Detection timestamp
   - `threat_level` - Threat classification

---

## API Endpoints

### 1. Historical Performance Analysis

**Endpoint**: `GET /research/statistical-analysis/historical`

Compare performance between two time periods using actual database data.

**Query Parameters**:
- `baseline_start` - Baseline period start timestamp (default: 7 days ago)
- `baseline_end` - Baseline period end timestamp (default: 3 days ago)
- `current_start` - Current period start timestamp (default: 3 days ago)
- `current_end` - Current period end timestamp (default: now)
- `bookmaker` - Filter by bookmaker (optional)
- `eventId` - Filter by event ID (optional)
- `limit` - Maximum samples per period (default: 1000)

**Example**:
```bash
# Compare last week vs this week
curl "http://localhost:3001/research/statistical-analysis/historical?baseline_start=$(date -d '7 days ago' +%s)000&baseline_end=$(date -d '3 days ago' +%s)000&current_start=$(date -d '3 days ago' +%s)000&current_end=$(date +%s)000"
```

**Response**:
```json
{
  "success": true,
  "analysis": {
    "meanDifference": {
      "test": {
        "pValue": 0.001,
        "isSignificant": true,
        "testStatistic": -4.2,
        "degreesOfFreedom": 38
      },
      "meanDiff": -5.3,
      "confidenceInterval": {
        "lower": -7.1,
        "upper": -3.5,
        "level": 0.95
      },
      "effectSize": {
        "value": -1.1,
        "magnitude": "large"
      }
    },
    "varianceComparison": { ... },
    "distributionComparison": { ... },
    "metadata": {
      "databasePath": "./data/research.db",
      "tableName": "line_movement_audit_v2",
      "queryTimeRange": { ... },
      "sampleCount": 2000
    }
  }
}
```

### 2. Anomaly Performance Impact Analysis

**Endpoint**: `GET /research/statistical-analysis/anomaly-impact`

Analyze the performance impact of URL anomalies vs normal operations.

**Query Parameters**:
- `bookmaker` - Filter by bookmaker (optional)
- `start_time` - Start timestamp (default: 7 days ago)
- `end_time` - End timestamp (default: now)
- `limit` - Maximum samples (default: 1000)

**Example**:
```bash
curl "http://localhost:3001/research/statistical-analysis/anomaly-impact?bookmaker=example-bookmaker&start_time=$(date -d '7 days ago' +%s)000"
```

### 3. Bookmaker Performance Comparison

**Endpoint**: `GET /research/statistical-analysis/bookmaker-comparison`

Compare performance across multiple bookmakers.

**Query Parameters**:
- `bookmakers` - Comma-separated list of bookmaker names (required, min 2)
- `start_time` - Start timestamp (default: 7 days ago)
- `end_time` - End timestamp (default: now)
- `limit` - Maximum samples per bookmaker (default: 1000)

**Example**:
```bash
curl "http://localhost:3001/research/statistical-analysis/bookmaker-comparison?bookmakers=bookmaker-a,bookmaker-b,bookmaker-c"
```

---

## Programmatic Usage

### Direct Database Access

```typescript
import { Database } from "bun:sqlite";
import {
	analyzeHistoricalPerformance,
	analyzeAnomalyPerformanceImpact,
	compareBookmakerPerformance,
} from "./src/utils/statistical-analysis-database";

const db = new Database("./data/research.db");

// Compare performance: last week vs this week
const weekAgo = Date.now() - 7 * 24 * 60 * 60 * 1000;
const threeDaysAgo = Date.now() - 3 * 24 * 60 * 60 * 1000;
const now = Date.now();

const analysis = await analyzeHistoricalPerformance(
	db,
	{
		startTime: weekAgo,
		endTime: threeDaysAgo,
		bookmaker: "example-bookmaker",
	},
	{
		startTime: threeDaysAgo,
		endTime: now,
		bookmaker: "example-bookmaker",
	},
);

if (analysis.meanDifference.test.isSignificant) {
	const effect = analysis.meanDifference.effectSize;
	console.log(`Performance regression detected: ${effect.magnitude} effect (d=${effect.value.toFixed(2)})`);
	console.log(`95% CI: [${analysis.meanDifference.confidenceInterval.lower.toFixed(2)}, ${analysis.meanDifference.confidenceInterval.upper.toFixed(2)}]`);
}

// Analyze anomaly impact
const anomalyAnalysis = await analyzeAnomalyPerformanceImpact(db, {
	bookmaker: "example-bookmaker",
	startTime: weekAgo,
	endTime: now,
});

// Compare bookmakers
const comparisons = await compareBookmakerPerformance(
	db,
	["bookmaker-a", "bookmaker-b", "bookmaker-c"],
	{ startTime: weekAgo, endTime: now },
);

for (const [bookmaker, comparison] of comparisons) {
	if (comparison.meanDifference.test.isSignificant) {
		console.log(`${bookmaker}: ${comparison.meanDifference.effectSize.magnitude} difference`);
	}
}

db.close();
```

---

## Use Cases

### 1. Performance Regression Detection

Detect if recent changes caused performance degradation:

```typescript
// Compare performance before and after deployment
const deploymentTime = 1234567890000; // Deployment timestamp

const analysis = await analyzeHistoricalPerformance(
	db,
	{ startTime: deploymentTime - 86400000, endTime: deploymentTime }, // Day before
	{ startTime: deploymentTime, endTime: deploymentTime + 86400000 }, // Day after
);

if (analysis.meanDifference.test.isSignificant && analysis.meanDifference.meanDiff < 0) {
	console.error("ðŸš¨ Performance regression detected after deployment!");
}
```

### 2. Anomaly Impact Assessment

Measure how URL anomalies affect system performance:

```typescript
const impact = await analyzeAnomalyPerformanceImpact(db, {
	bookmaker: "problematic-bookmaker",
	startTime: Date.now() - 7 * 24 * 60 * 60 * 1000,
});

if (impact.meanDifference.test.isSignificant) {
	console.log(`Anomalies cause ${impact.meanDifference.effectSize.magnitude} performance impact`);
	console.log(`Average slowdown: ${Math.abs(impact.meanDifference.meanDiff).toFixed(2)}ms`);
}
```

### 3. Bookmaker Performance Benchmarking

Compare performance across different bookmakers:

```typescript
const comparisons = await compareBookmakerPerformance(
	db,
	["bookmaker-a", "bookmaker-b", "bookmaker-c"],
	{ startTime: Date.now() - 24 * 60 * 60 * 1000 }, // Last 24 hours
);

// Find fastest bookmaker
let fastest = "bookmaker-a";
let fastestTime = Infinity;

for (const [bookmaker, comparison] of comparisons) {
	const avgTime = comparison.meanDifference.meanDiff;
	if (avgTime < fastestTime) {
		fastestTime = avgTime;
		fastest = bookmaker;
	}
}

console.log(`Fastest bookmaker: ${fastest}`);
```

### 4. Time-Series Performance Monitoring

Track performance trends over time:

```typescript
// Compare performance week-over-week
const now = Date.now();
const weeks = 4;

for (let week = 0; week < weeks; week++) {
	const weekStart = now - (week + 1) * 7 * 24 * 60 * 60 * 1000;
	const weekEnd = now - week * 7 * 24 * 60 * 60 * 1000;
	const prevWeekStart = weekStart - 7 * 24 * 60 * 60 * 1000;
	const prevWeekEnd = weekStart;

	const analysis = await analyzeHistoricalPerformance(
		db,
		{ startTime: prevWeekStart, endTime: prevWeekEnd },
		{ startTime: weekStart, endTime: weekEnd },
	);

	console.log(`Week ${week + 1}: ${analysis.meanDifference.effectSize.magnitude} change`);
}
```

---

## Performance Metrics Extraction

### From `line_movement_audit_v2`

The adapter extracts performance metrics using:

1. **Response Size as Execution Time Proxy**
   - Converts `response_size` (bytes) to estimated execution time
   - Approximation: 1KB â‰ˆ 1ms processing time
   - Formula: `executionTime = response_size / 1024`

2. **Timestamp Deltas**
   - Calculates time between consecutive requests
   - Used when `response_size` is unavailable
   - Filters out unrealistic deltas (> 60 seconds)

### From `url_anomaly_audit`

The adapter uses:

1. **Parameter Count Deltas**
   - Difference between parsed and corrected parameter counts
   - Indicates processing overhead from URL parsing anomalies
   - Formula: `executionTime = abs(param_delta) * 0.5ms`

---

## Statistical Analysis Results

All endpoints return complete statistical analysis including:

### Mean Difference Analysis
- **t-test / Welch's t-test**: Detects significant differences
- **Confidence Interval**: Range estimate for true difference
- **Effect Size (Cohen's d)**: Magnitude classification

### Variance Comparison
- **F-test**: Detects variance changes
- **Variance Ratio**: Quantifies variance difference

### Distribution Comparison
- **Kolmogorov-Smirnov Test**: Detects distribution shape changes

### Metadata
- Database path and table name
- Query time range
- Sample counts
- Filter criteria

---

## Best Practices

1. **Sufficient Sample Sizes**: Aim for at least 20-30 samples per period for robust analysis

2. **Meaningful Time Ranges**: Use comparable time periods (e.g., same day of week, same duration)

3. **Filter Appropriately**: Use `bookmaker` and `eventId` filters to isolate specific systems

4. **Interpret Results Correctly**:
   - Statistical significance â‰  practical importance
   - Check effect size to determine practical impact
   - Consider confidence intervals for uncertainty

5. **Monitor Trends**: Use time-series analysis to track performance over time

---

## Integration with Existing Systems

### CPU Profiling (6.7.1A.0.0.0.0)

The database integration complements CPU profiling:

- **CPU Profiling**: Real-time performance analysis from `.cpuprofile` files
- **Database Analysis**: Historical performance trends from audit logs
- **Combined**: Complete performance picture across time

### URL Anomaly Detection

The database integration enables:

- **Anomaly Impact**: Measure performance cost of URL parsing anomalies
- **Pattern Correlation**: Correlate anomaly patterns with performance regressions
- **Optimization**: Identify which anomalies cause the most performance impact

---

## Testing

Run the database integration tests:

```bash
bun test test/statistical-analysis-database.test.ts
```

Run all 6.7.1A.0.0.0.0 tests:

```bash
bun test test/cpu-profiling-statistics*.test.ts test/statistical-analysis-database.test.ts
```

---

## Status

âœ… **Database Integration**: Complete  
âœ… **API Endpoints**: Operational  
âœ… **Test Coverage**: Comprehensive  
âœ… **Documentation**: Complete  

**6.7.1A.0.0.0.0 can now confidently access raw performance data from database audit tables and perform statistical comparisons on actual historical data.**
